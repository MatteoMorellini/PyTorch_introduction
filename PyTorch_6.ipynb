{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Going Modular!\n",
        "Turn the notebook code into Python scripts saved as files in a directory:\n",
        "* `data_setup.py` - prepare and download data\n",
        "* `engine.py` - several training functions\n",
        "* `model_builder.py` - create a PyTorch model\n",
        "* `train.py` - leverage all other scripts\n",
        "* `utils.py` - helper functions"
      ],
      "metadata": {
        "id": "auX2r7riLR1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main pros of Python scripts:\n",
        "* By importing modules we can avoid rewriting similar code across different notebooks\n",
        "* Git for versioning, which can't be used for notebooks\n",
        "* Can run very specific parts\n"
      ],
      "metadata": {
        "id": "dmBnUD5GWufQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cons:\n",
        "* Experimenting isn't as visual\n",
        "* More step to share"
      ],
      "metadata": {
        "id": "EoEhfNrNXNRO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run a script**\n",
        "Running a PyTorch train.py script on the command line with various hyperparameter settings could look like this:\n",
        "\n",
        "\n",
        "```\n",
        "python train.py --model MODEL_NAME --batch_size BATCH_SIZE --lr LEARNING_RATE\n",
        "```\n",
        "where `--model, --batch_size...` are called **argument flags**\n"
      ],
      "metadata": {
        "id": "JsDW3WyAXf8W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Module or Script**\n",
        "<br>Module:\n",
        "* file containing Python code intended to be **imported**  and used by other Python files or scripts. When imported, the code is executed once\n",
        "* generally used to encapsulate and organize code (functions, classes...) so they can be re-used\n",
        "* typically not executed directly\n"
      ],
      "metadata": {
        "id": "9s3SGWkZZ6x8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Script:\n",
        "- file intended to be executed directly as a standalone program to perform a specific task, often from the command line\n",
        "- their primary purpose is to perform an action rather than be re-used as part of a larger system"
      ],
      "metadata": {
        "id": "MZge3YHKacEh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Python coding style**\n",
        "<br> Each of the functions and classes is created following **Google's Python docstring** style, also all modules are imported at the top of the script since it's easier to understand what will happen\n",
        "<br>\n",
        "A Python docstring (*documentation string*) is a special type of string (enclosed in triple quotes) used to document a specific segment of code (function, method, class or module): what it does, how it works and how it should be used."
      ],
      "metadata": {
        "id": "9yCLQe2Aa8IS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example of a Function Docstring\n",
        "\n",
        "def add(a, b):\n",
        "    \"\"\"\n",
        "    Adds two numbers together.\n",
        "\n",
        "    Args:\n",
        "        a (int or float): The first number.\n",
        "        b (int or float): The second number.\n",
        "\n",
        "    Returns:\n",
        "        int or float: The sum of the two numbers. For example:\n",
        "        3\n",
        "    \"\"\"\n",
        "    return a + b\n"
      ],
      "metadata": {
        "id": "nIHGTVYxcgnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example of a Class Docstring\n",
        "\n",
        "class MyClass:\n",
        "    \"\"\"\n",
        "    This is a sample class.\n",
        "\n",
        "    Attributes:\n",
        "        attribute1 (str): Description of attribute1.\n",
        "        attribute2 (int): Description of attribute2.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, attribute1, attribute2):\n",
        "        self.attribute1 = attribute1\n",
        "        self.attribute2 = attribute2\n"
      ],
      "metadata": {
        "id": "hW2GlvXwXLbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxEyZqWKKk01",
        "outputId": "144b9370-44c5-4d38-a87b-480897a70d7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    This is a sample class.\n",
            "\n",
            "    Attributes:\n",
            "        attribute1 (str): Description of attribute1.\n",
            "        attribute2 (int): Description of attribute2.\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "print(MyClass.__doc__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# Setup path to data folder\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"pizza_steak_sushi\"\n",
        "\n",
        "# If the image folder doesn't exist, download it and prepare it...\n",
        "if image_path.is_dir():\n",
        "    print(f\"{image_path} directory exists.\")\n",
        "else:\n",
        "    print(f\"Did not find {image_path} directory, creating one...\")\n",
        "    image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Download pizza, steak, sushi data\n",
        "with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
        "    request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
        "    print(\"Downloading pizza, steak, sushi data...\")\n",
        "    f.write(request.content)\n",
        "\n",
        "# Unzip pizza, steak, sushi data\n",
        "with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
        "    print(\"Unzipping pizza, steak, sushi data...\")\n",
        "    zip_ref.extractall(image_path)\n",
        "\n",
        "# Remove zip file\n",
        "os.remove(data_path / \"pizza_steak_sushi.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ft3OWmV5i3eK",
        "outputId": "f65de1fa-829d-4aba-ce10-69461a86c178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Did not find data/pizza_steak_sushi directory, creating one...\n",
            "Downloading pizza, steak, sushi data...\n",
            "Unzipping pizza, steak, sushi data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Magic command: %%writefile** <br>\n",
        "The %%writefile command is used in Notebooks to write the contents of the cell to a file.\n",
        "In this case, it creates a file named text.py inside the example directory\n",
        "\n"
      ],
      "metadata": {
        "id": "JLClWOTBjcLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir going_modular"
      ],
      "metadata": {
        "id": "XGNKwZZKjZ3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/text.py\n",
        "def prova():\n",
        "    return 'ciao mare'"
      ],
      "metadata": {
        "id": "t5kGwvioc498",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36d3197e-bea5-4a30-9559-d1f72debedad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/text.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/data_setup.py\n",
        "\"\"\"\n",
        "Contains functionality for creating PyTorch DataLoaders for\n",
        "image classification data.\n",
        "\"\"\"\n",
        "import os\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "def create_dataloaders(\n",
        "    train_dir: str,\n",
        "    test_dir: str,\n",
        "    transform: transforms.Compose,\n",
        "    batch_size: int,\n",
        "    num_workers: int=NUM_WORKERS\n",
        "):\n",
        "  \"\"\"Creates training and testing DataLoaders.\n",
        "\n",
        "  Takes in a training directory and testing directory path and turns\n",
        "  them into PyTorch Datasets and then into PyTorch DataLoaders.\n",
        "\n",
        "  Args:\n",
        "    train_dir: Path to training directory.\n",
        "    test_dir: Path to testing directory.\n",
        "    transform: torchvision transforms to perform on training and testing data.\n",
        "    batch_size: Number of samples per batch in each of the DataLoaders.\n",
        "    num_workers: An integer for number of workers per DataLoader.\n",
        "\n",
        "  Returns:\n",
        "    A tuple of (train_dataloader, test_dataloader, class_names).\n",
        "    Where class_names is a list of the target classes.\n",
        "    Example usage:\n",
        "      train_dataloader, test_dataloader, class_names = \\\n",
        "        = create_dataloaders(train_dir=path/to/train_dir,\n",
        "                             test_dir=path/to/test_dir,\n",
        "                             transform=some_transform,\n",
        "                             batch_size=32,\n",
        "                             num_workers=4)\n",
        "  \"\"\"\n",
        "  # Use ImageFolder to create dataset(s)\n",
        "  train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
        "  test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
        "\n",
        "  # Get class names\n",
        "  class_names = train_data.classes\n",
        "\n",
        "  # Turn images into data loaders\n",
        "  train_dataloader = DataLoader(\n",
        "      train_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=True,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "  test_dataloader = DataLoader(\n",
        "      test_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False, # don't need to shuffle test data\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "\n",
        "  return train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xx4QDWfLiHke",
        "outputId": "5b98428f-f45b-4366-fcf5-c0e0bdeef260"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/data_setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/model_builder.py\n",
        "\"\"\"\n",
        "Contains PyTorch model code to instantiate a TinyVGG model.\n",
        "\"\"\"\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class TinyVGG(nn.Module):\n",
        "  \"\"\"Creates the TinyVGG architecture.\n",
        "\n",
        "  Replicates the TinyVGG architecture from the CNN explainer website in PyTorch.\n",
        "  See the original architecture here: https://poloclub.github.io/cnn-explainer/\n",
        "\n",
        "  Args:\n",
        "    input_shape: An integer indicating number of input channels.\n",
        "    hidden_units: An integer indicating number of hidden units between layers.\n",
        "    output_shape: An integer indicating number of output units.\n",
        "  \"\"\"\n",
        "  def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
        "      super().__init__()\n",
        "      self.conv_block_1 = nn.Sequential(\n",
        "          nn.Conv2d(in_channels=input_shape,\n",
        "                    out_channels=hidden_units,\n",
        "                    kernel_size=3,\n",
        "                    stride=1,\n",
        "                    padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(in_channels=hidden_units,\n",
        "                    out_channels=hidden_units,\n",
        "                    kernel_size=3,\n",
        "                    stride=1,\n",
        "                    padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(kernel_size=2,\n",
        "                        stride=2)\n",
        "      )\n",
        "      self.conv_block_2 = nn.Sequential(\n",
        "          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(2)\n",
        "      )\n",
        "      self.classifier = nn.Sequential(\n",
        "          nn.Flatten(),\n",
        "          # Where did this in_features shape come from?\n",
        "          # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
        "          nn.Linear(in_features=hidden_units*13*13,\n",
        "                    out_features=output_shape)\n",
        "      )\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "      x = self.conv_block_1(x)\n",
        "      x = self.conv_block_2(x)\n",
        "      x = self.classifier(x)\n",
        "      return x\n",
        "      # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # <- leverage the benefits of operator fusion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POy10onyjGME",
        "outputId": "25921be0-03b3-4b5a-e82f-595bc5cda642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/model_builder.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Make only certain classes (or functions, variables, etc.) available for import**<br>\n",
        "- using naming conventions - names that start with a single underscore (e.g., `_PrivateClass`) are treated as non-public and are not imported when using `from module import *`. However, they can still be imported explicitly, e.g., `from module import _PrivateClass`.\n",
        "- `__all__` - this variable is a list of strings that specifies which classes, functions, or variables should be considered \"public\" and available when the module is imported using the `from module import * syntax`. It **always** prevent a class/function from being exported"
      ],
      "metadata": {
        "id": "PbqtRvjFlPFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# my_module.py\n",
        "\n",
        "class PublicClass1:\n",
        "    def __init__(self):\n",
        "        print(\"PublicClass1\")\n",
        "\n",
        "class PublicClass2:\n",
        "    def __init__(self):\n",
        "        print(\"PublicClass2\")\n",
        "\n",
        "class _PrivateClass:\n",
        "    def __init__(self):\n",
        "        print(\"PrivateClass\")\n",
        "\n",
        "__all__ = [\"PublicClass1\", \"PublicClass2\"]  # Only these classes will be available\n"
      ],
      "metadata": {
        "id": "t1s7SXUllOkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from my_module import *\n",
        "\n",
        "obj1 = PublicClass1()  # This will work\n",
        "obj2 = PublicClass2()  # This will work\n",
        "obj3 = _PrivateClass() # This will raise an ImportError"
      ],
      "metadata": {
        "id": "xKW7_K-EmyAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# my_module.py\n",
        "\n",
        "class PublicClass:\n",
        "    pass\n",
        "\n",
        "class _PrivateClass:\n",
        "    pass"
      ],
      "metadata": {
        "id": "l9dG2jwTm1yZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from my_module import * # Import only PublicClass\n",
        "from my_module import PublicClass  # This works\n",
        "from my_module import _PrivateClass  # This also works but is discouraged"
      ],
      "metadata": {
        "id": "ER22J_BkmtXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile module_1.py\n",
        "\n",
        "class PublicClass1:\n",
        "    def __init__(self):\n",
        "        print(\"PublicClass1\")\n",
        "\n",
        "class PublicClass2:\n",
        "    def __init__(self):\n",
        "        print(\"PublicClass2\")\n",
        "\n",
        "class _PrivateClass1:\n",
        "    def __init__(self):\n",
        "        print(\"PrivateClass\")\n",
        "\n",
        "__all__ = [\"PublicClass1\", \"PublicClass2\"]  # Only these classes will be available\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBB49BCttcy5",
        "outputId": "c2984be2-8d46-4e1b-a8e0-eae6c23e78be"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting module_1.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from module_1 import * # Import only PublicClass"
      ],
      "metadata": {
        "id": "Kzk__Jjttev4"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_PrivateClass1()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "Jiwg-eocthvn",
        "outputId": "5f52fd71-e3b5-4803-e986-274fb966badb"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name '_PrivateClass1' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-b6d0326ae4f8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_PrivateClass1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name '_PrivateClass1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from module_1 import _PrivateClass1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "EOFwGq7PuLZF",
        "outputId": "e47a269e-5944-4963-861d-d2cad051d5f2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name '_PrivateClass1' from 'module_1' (/content/module_1.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-5c2ced6a02c7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodule_1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_PrivateClass1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name '_PrivateClass1' from 'module_1' (/content/module_1.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile module_2.py\n",
        "\n",
        "class PublicClass1:\n",
        "    def __init__(self):\n",
        "        print(\"PublicClass1\")\n",
        "\n",
        "class PublicClass2:\n",
        "    def __init__(self):\n",
        "        print(\"PublicClass2\")\n",
        "\n",
        "class _PrivateClass2:\n",
        "    def __init__(self):\n",
        "        print(\"PrivateClass\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucclPsoPufvX",
        "outputId": "fccd2661-6189-4d93-d04c-fb856ff3b767"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing module_2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from module_2 import *\n",
        "_PrivateClass2()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "_U8JDaDru16N",
        "outputId": "ee0831bf-3c5f-4fe4-a690-fbc9015433f0"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name '_PrivateClass2' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-abae2e6cf5a1>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodule_2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0m_PrivateClass2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name '_PrivateClass2' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from module_2 import _PrivateClass2\n",
        "_PrivateClass2()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euwFAnlKu6Bu",
        "outputId": "7a27efa7-9f86-46ce-fa5b-813f98bc7386"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PrivateClass\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module_2._PrivateClass2 at 0x7c16185401c0>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/engine.py\n",
        "\"\"\"\n",
        "Contains functions for training and testing a PyTorch model.\n",
        "\"\"\"\n",
        "import torch\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "def _train_step(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               device: torch.device) -> Tuple[float, float]:\n",
        "  \"\"\"Trains a PyTorch model for a single epoch.\n",
        "\n",
        "  Turns a target PyTorch model to training mode and then\n",
        "  runs through all of the required training steps (forward\n",
        "  pass, loss calculation, optimizer step).\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be trained.\n",
        "    dataloader: A DataLoader instance for the model to be trained on.\n",
        "    loss_fn: A PyTorch loss function to minimize.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "    A tuple of training loss and training accuracy metrics.\n",
        "    In the form (train_loss, train_accuracy). For example:\n",
        "\n",
        "    (0.1112, 0.8743)\n",
        "  \"\"\"\n",
        "  # Put model in train mode\n",
        "  model.train()\n",
        "\n",
        "  # Setup train loss and train accuracy values\n",
        "  train_loss, train_acc = 0, 0\n",
        "\n",
        "  # Loop through data loader data batches\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "      # Send data to target device\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      # 1. Forward pass\n",
        "      y_pred = model(X)\n",
        "\n",
        "      # 2. Calculate  and accumulate loss\n",
        "      loss = loss_fn(y_pred, y)\n",
        "      train_loss += loss.item()\n",
        "\n",
        "      # 3. Optimizer zero grad\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # 4. Loss backward\n",
        "      loss.backward()\n",
        "\n",
        "      # 5. Optimizer step\n",
        "      optimizer.step()\n",
        "\n",
        "      # Calculate and accumulate accuracy metric across all batches\n",
        "      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "      train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
        "\n",
        "  # Adjust metrics to get average loss and accuracy per batch\n",
        "  train_loss = train_loss / len(dataloader)\n",
        "  train_acc = train_acc / len(dataloader)\n",
        "  return train_loss, train_acc\n",
        "\n",
        "def _test_step(model: torch.nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              device: torch.device) -> Tuple[float, float]:\n",
        "  \"\"\"Tests a PyTorch model for a single epoch.\n",
        "\n",
        "  Turns a target PyTorch model to \"eval\" mode and then performs\n",
        "  a forward pass on a testing dataset.\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be tested.\n",
        "    dataloader: A DataLoader instance for the model to be tested on.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on the test data.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "    A tuple of testing loss and testing accuracy metrics.\n",
        "    In the form (test_loss, test_accuracy). For example:\n",
        "\n",
        "    (0.0223, 0.8985)\n",
        "  \"\"\"\n",
        "  # Put model in eval mode\n",
        "  model.eval()\n",
        "\n",
        "  # Setup test loss and test accuracy values\n",
        "  test_loss, test_acc = 0, 0\n",
        "\n",
        "  # Turn on inference context manager\n",
        "  with torch.inference_mode():\n",
        "      # Loop through DataLoader batches\n",
        "      for batch, (X, y) in enumerate(dataloader):\n",
        "          # Send data to target device\n",
        "          X, y = X.to(device), y.to(device)\n",
        "\n",
        "          # 1. Forward pass\n",
        "          test_pred_logits = model(X)\n",
        "\n",
        "          # 2. Calculate and accumulate loss\n",
        "          loss = loss_fn(test_pred_logits, y)\n",
        "          test_loss += loss.item()\n",
        "\n",
        "          # Calculate and accumulate accuracy\n",
        "          test_pred_labels = test_pred_logits.argmax(dim=1)\n",
        "          test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
        "\n",
        "  # Adjust metrics to get average loss and accuracy per batch\n",
        "  test_loss = test_loss / len(dataloader)\n",
        "  test_acc = test_acc / len(dataloader)\n",
        "  return test_loss, test_acc\n",
        "\n",
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          epochs: int,\n",
        "          device: torch.device) -> Dict[str, List]:\n",
        "  \"\"\"Trains and tests a PyTorch model.\n",
        "\n",
        "  Passes a target PyTorch models through train_step() and test_step()\n",
        "  functions for a number of epochs, training and testing the model\n",
        "  in the same epoch loop.\n",
        "\n",
        "  Calculates, prints and stores evaluation metrics throughout.\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be trained and tested.\n",
        "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
        "    test_dataloader: A DataLoader instance for the model to be tested on.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
        "    epochs: An integer indicating how many epochs to train for.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "    A dictionary of training and testing loss as well as training and\n",
        "    testing accuracy metrics. Each metric has a value in a list for\n",
        "    each epoch.\n",
        "    In the form: {train_loss: [...],\n",
        "                  train_acc: [...],\n",
        "                  test_loss: [...],\n",
        "                  test_acc: [...]}\n",
        "    For example if training for epochs=2:\n",
        "                 {train_loss: [2.0616, 1.0537],\n",
        "                  train_acc: [0.3945, 0.3945],\n",
        "                  test_loss: [1.2641, 1.5706],\n",
        "                  test_acc: [0.3400, 0.2973]}\n",
        "  \"\"\"\n",
        "  # Create empty results dictionary\n",
        "  results = {\"train_loss\": [],\n",
        "      \"train_acc\": [],\n",
        "      \"test_loss\": [],\n",
        "      \"test_acc\": []\n",
        "  }\n",
        "\n",
        "  # Loop through training and testing steps for a number of epochs\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "      train_loss, train_acc = _train_step(model=model,\n",
        "                                          dataloader=train_dataloader,\n",
        "                                          loss_fn=loss_fn,\n",
        "                                          optimizer=optimizer,\n",
        "                                          device=device)\n",
        "      test_loss, test_acc = _test_step(model=model,\n",
        "          dataloader=test_dataloader,\n",
        "          loss_fn=loss_fn,\n",
        "          device=device)\n",
        "\n",
        "      # Print out what's happening\n",
        "      print(\n",
        "          f\"Epoch: {epoch+1} | \"\n",
        "          f\"train_loss: {train_loss:.4f} | \"\n",
        "          f\"train_acc: {train_acc:.4f} | \"\n",
        "          f\"test_loss: {test_loss:.4f} | \"\n",
        "          f\"test_acc: {test_acc:.4f}\"\n",
        "      )\n",
        "\n",
        "      # Update results dictionary\n",
        "      results[\"train_loss\"].append(train_loss)\n",
        "      results[\"train_acc\"].append(train_acc)\n",
        "      results[\"test_loss\"].append(test_loss)\n",
        "      results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "  # Return the filled results at the end of the epochs\n",
        "  return results\n",
        "\n",
        "__all__=['train']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPvBD-o5npJd",
        "outputId": "a542762b-efca-41e6-94da-a1d34e37ade5"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting going_modular/engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/utils.py\n",
        "\"\"\"\n",
        "Contains various utility functions for PyTorch model training and saving.\n",
        "\"\"\"\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "def save_model(model: torch.nn.Module,\n",
        "               target_dir: str,\n",
        "               model_name: str):\n",
        "  \"\"\"Saves a PyTorch model to a target directory.\n",
        "\n",
        "  Args:\n",
        "    model: A target PyTorch model to save.\n",
        "    target_dir: A directory for saving the model to.\n",
        "    model_name: A filename for the saved model. Should include\n",
        "      either \".pth\" or \".pt\" as the file extension.\n",
        "\n",
        "  Example usage:\n",
        "    save_model(model=model_0,\n",
        "               target_dir=\"models\",\n",
        "               model_name=\"05_going_modular_tingvgg_model.pth\")\n",
        "  \"\"\"\n",
        "  # Create target directory\n",
        "  target_dir_path = Path(target_dir)\n",
        "  target_dir_path.mkdir(parents=True,\n",
        "                        exist_ok=True)\n",
        "\n",
        "  # Create model save path\n",
        "  assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n",
        "  model_save_path = target_dir_path / model_name\n",
        "\n",
        "  # Save the model state_dict()\n",
        "  print(f\"[INFO] Saving model to: {model_save_path}\")\n",
        "  torch.save(obj=model.state_dict(),\n",
        "             f=model_save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QC_hB-tfu-g7",
        "outputId": "f0afea08-f630-40a8-d5e2-9a3418890971"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/train.py\n",
        "\"\"\"\n",
        "Trains a PyTorch image classification model using device-agnostic code.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "# Since the modules are in the same folder, no need to \"from going_modular import...\"\n",
        "import data_setup, engine, model_builder, utils\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "import argparse\n",
        "\n",
        "def _parse_arguments():\n",
        "    \"\"\"\n",
        "    Sets up argument parsing and returns the parsed arguments.\n",
        "    \"\"\"\n",
        "    parser = argparse.ArgumentParser(description=\"A simple argument parser example.\")\n",
        "\n",
        "    # Add arguments\n",
        "    parser.add_argument('--num_epochs', type=int, default=5, help='Number of epochs the model will be trained on')\n",
        "    parser.add_argument('--batch_size', type=int, default=32, help='Number of samples per batch')\n",
        "    parser.add_argument('--hidden_units', type=int, default=10, help='Neurons in the hidden layer')\n",
        "    parser.add_argument('--lr', type=float, default=0.001, help='Learning rate')\n",
        "\n",
        "    # Parse arguments\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    return args\n",
        "\n",
        "def main(args):\n",
        "\n",
        "    # Setup hyperparameters\n",
        "    NUM_EPOCHS = args.num_epochs\n",
        "    BATCH_SIZE = args.batch_size\n",
        "    HIDDEN_UNITS = args.hidden_units\n",
        "    LEARNING_RATE = args.lr\n",
        "\n",
        "    # Setup directories\n",
        "    train_dir = \"data/pizza_steak_sushi/train\"\n",
        "    test_dir = \"data/pizza_steak_sushi/test\"\n",
        "\n",
        "    # Setup target device\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Create transforms\n",
        "    data_transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    # Create DataLoaders with help from data_setup.py\n",
        "    train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
        "        train_dir=train_dir,\n",
        "        test_dir=test_dir,\n",
        "        transform=data_transform,\n",
        "        batch_size=BATCH_SIZE\n",
        "    )\n",
        "\n",
        "    # Create model with help from model_builder.py\n",
        "    model = model_builder.TinyVGG(\n",
        "        input_shape=3,\n",
        "        hidden_units=HIDDEN_UNITS,\n",
        "        output_shape=len(class_names)\n",
        "    ).to(device)\n",
        "\n",
        "    # Set loss and optimizer\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(),\n",
        "                                lr=LEARNING_RATE)\n",
        "\n",
        "    # Start training with help from engine.py\n",
        "    engine.train(model=model,\n",
        "                train_dataloader=train_dataloader,\n",
        "                test_dataloader=test_dataloader,\n",
        "                loss_fn=loss_fn,\n",
        "                optimizer=optimizer,\n",
        "                epochs=NUM_EPOCHS,\n",
        "                device=device)\n",
        "\n",
        "    # Save the model with help from utils.py\n",
        "    utils.save_model(model=model,\n",
        "                    target_dir=\"models\",\n",
        "                    model_name=\"05_going_modular_script_mode_tinyvgg_model.pth\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = _parse_arguments()\n",
        "    main(args)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytw48FIS2M3r",
        "outputId": "774d476d-9401-4e02-c35f-d16e73e2ec1d"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting going_modular/train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python ./going_modular/train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYioKDtR2NSd",
        "outputId": "f3e4fecd-b47b-4b37-a650-8cea0941f493"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0% 0/5 [00:00<?, ?it/s]Epoch: 1 | train_loss: 1.0981 | train_acc: 0.3945 | test_loss: 1.0932 | test_acc: 0.5938\n",
            " 20% 1/5 [00:02<00:11,  2.77s/it]Epoch: 2 | train_loss: 1.0794 | train_acc: 0.5430 | test_loss: 1.0950 | test_acc: 0.2699\n",
            " 40% 2/5 [00:05<00:07,  2.53s/it]Epoch: 3 | train_loss: 1.0931 | train_acc: 0.2969 | test_loss: 1.0895 | test_acc: 0.3201\n",
            " 60% 3/5 [00:06<00:04,  2.14s/it]Epoch: 4 | train_loss: 1.0730 | train_acc: 0.4258 | test_loss: 1.0748 | test_acc: 0.5028\n",
            " 80% 4/5 [00:08<00:01,  1.95s/it]Epoch: 5 | train_loss: 1.0511 | train_acc: 0.4688 | test_loss: 1.0595 | test_acc: 0.3627\n",
            "100% 5/5 [00:10<00:00,  2.02s/it]\n",
            "[INFO] Saving model to: models/05_going_modular_script_mode_tinyvgg_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python ./going_modular/train.py --num_epochs 10 --batch_size 64"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vWGqvrO-NgU",
        "outputId": "d7871fc6-1e4a-4ddb-fdfd-e7910b2ed823"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r  0% 0/10 [00:00<?, ?it/s]0\n",
            "1\n",
            "2\n",
            "3\n",
            "Epoch: 1 | train_loss: 1.0985 | train_acc: 0.3326 | test_loss: 1.1091 | test_acc: 0.2188\n",
            " 10% 1/10 [00:02<00:18,  2.10s/it]0\n",
            "1\n",
            "2\n",
            "3\n",
            "Epoch: 2 | train_loss: 1.0906 | train_acc: 0.3917 | test_loss: 1.0895 | test_acc: 0.1953\n",
            " 20% 2/10 [00:04<00:19,  2.45s/it]0\n",
            "1\n",
            "2\n",
            "3\n",
            "Epoch: 3 | train_loss: 1.0785 | train_acc: 0.3565 | test_loss: 1.0937 | test_acc: 0.2188\n",
            " 30% 3/10 [00:06<00:15,  2.14s/it]0\n",
            "1\n",
            "2\n",
            "3\n",
            "Epoch: 4 | train_loss: 1.0544 | train_acc: 0.4537 | test_loss: 1.0753 | test_acc: 0.2109\n",
            " 40% 4/10 [00:08<00:12,  2.01s/it]0\n",
            "1\n",
            "2\n",
            "3\n",
            "Epoch: 5 | train_loss: 1.0127 | train_acc: 0.5621 | test_loss: 1.0227 | test_acc: 0.3473\n",
            " 50% 5/10 [00:10<00:09,  1.93s/it]0\n",
            "1\n",
            "2\n",
            "3\n",
            "Epoch: 6 | train_loss: 0.9529 | train_acc: 0.6093 | test_loss: 1.0433 | test_acc: 0.3551\n",
            " 60% 6/10 [00:11<00:07,  1.89s/it]0\n",
            "1\n",
            "2\n",
            "3\n",
            "Epoch: 7 | train_loss: 0.8947 | train_acc: 0.6244 | test_loss: 1.0100 | test_acc: 0.3395\n",
            " 70% 7/10 [00:13<00:05,  1.86s/it]0\n",
            "1\n",
            "2\n",
            "3\n",
            "Epoch: 8 | train_loss: 0.8690 | train_acc: 0.6235 | test_loss: 1.1466 | test_acc: 0.2642\n",
            " 80% 8/10 [00:16<00:04,  2.09s/it]0\n",
            "1\n",
            "2\n",
            "3\n",
            "Epoch: 9 | train_loss: 0.8692 | train_acc: 0.6081 | test_loss: 0.9433 | test_acc: 0.5980\n",
            " 90% 9/10 [00:18<00:02,  2.16s/it]0\n",
            "1\n",
            "2\n",
            "3\n",
            "Epoch: 10 | train_loss: 0.8333 | train_acc: 0.5460 | test_loss: 0.9987 | test_acc: 0.4538\n",
            "100% 10/10 [00:20<00:00,  2.05s/it]\n",
            "[INFO] Saving model to: models/05_going_modular_script_mode_tinyvgg_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WpePj9Cl-i55"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}